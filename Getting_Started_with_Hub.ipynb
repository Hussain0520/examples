{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Getting Started with Hub 2.0",
      "provenance": [],
      "collapsed_sections": [
        "lKU8kmSs65xv",
        "ZrjGQON37lk2",
        "0N-f2SYU7OjQ",
        "bR5n8yYg-0Wu",
        "G-DM6PKq_di2",
        "46H4nEnZDv5m",
        "JGo-E8Z8Ho6F",
        "A8Mye_Z5Htut",
        "1Kb9q_ZqIARN",
        "bjmnRLWHINXG",
        "NQipSo2OF_lB",
        "LVma__gxGq97",
        "Bnr9ItdkGzDk",
        "x5bX92ZUG_2F",
        "guao84xTb4Zg",
        "iXRCphquSFs3",
        "_CEF-kjySdLp",
        "kWvgUH25Tj8V",
        "2JRpqeYqV-oT",
        "HCrKgp6FYDG9",
        "i1GqH1JvYkNP",
        "EnTPLIS5i7yE",
        "uinXs4r1i7Zz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/activeloopai/examples/blob/istranic-adding-colabs/Getting_Started_with_Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# **Step 1**: _Hello World_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrjGQON37lk2"
      },
      "source": [
        "## Installing Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pcfYcPu7KxY"
      },
      "source": [
        "Hub can be installed via `pip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC_N5qOx6o0d"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip3 install hub\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWJeNh83XfrD"
      },
      "source": [
        "# IMPORTANT - Please restart your Colab runtime after installing Hub!\n",
        "# This is a Colab-specific issue that prevents some imports from working properly.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N-f2SYU7OjQ"
      },
      "source": [
        "## Fetching your first Hub dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aNFn7rZ7qxP"
      },
      "source": [
        "Begin by loading in [MNIST](https://en.wikipedia.org/wiki/MNIST_database), the hello world dataset of machine learning. \n",
        "\n",
        "First, load the `Dataset` by pointing to its storage location. Datasets hosted on the Activeloop Platform are typically identified by the namespace of the organization followed by the dataset name: `activeloop/mnist-train`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izccjS4k7NvX"
      },
      "source": [
        "import hub\n",
        "\n",
        "dataset_path = 'hub://activeloop/mnist-train'\n",
        "ds = hub.load(dataset_path) # Returns a Hub Dataset but does not download data locally"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR5n8yYg-0Wu"
      },
      "source": [
        "## Reading Samples From a Hub Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdaAKaS-3NO"
      },
      "source": [
        "Data is not immediately read into memory because Hub operates [lazily](https://en.wikipedia.org/wiki/Lazy_evaluation). You can fetch data by calling the `.numpy()` method, which reads data into a NumPy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qpQeNoq-xfo"
      },
      "source": [
        "# Indexing\n",
        "W = ds.images[0].numpy() # Fetch image return a NumPy array\n",
        "X = ds.labels[0].numpy(aslist=True) # Fetch label and store as list of NumPy array\n",
        "\n",
        "# Slicing\n",
        "Y = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array if possible\n",
        "                               # This method produces an exception if\n",
        "                               # the shape of the images is not equal\n",
        "Z = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store as list of \n",
        "                                           # NumPy arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNGHXfdKwJ7W"
      },
      "source": [
        "print('X is {}'.format(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmi2w0_e_LtH"
      },
      "source": [
        "Congratulations, you've got Hub working on your local machine! ðŸ¤“"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-DM6PKq_di2"
      },
      "source": [
        "# **Step 2**: _Creating Hub Datasets_\n",
        "*Creating and storing Hub Datasets manually.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEzK8LTe_gJW"
      },
      "source": [
        "Creating Hub datasets is simple, you have full control over connecting your source data (files, images, etc.) to specific tensors in the Hub Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGXGvKU1qsp1"
      },
      "source": [
        "## Manual Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQk29Mnhqn1V"
      },
      "source": [
        "Let's follow along with the example below to create our first dataset. First, download and unzip the small classification dataset below called the *animals dataset*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDJRrlDP_DsW"
      },
      "source": [
        "# Download dataset\n",
        "from IPython.display import clear_output\n",
        "!wget https://firebasestorage.googleapis.com/v0/b/gitbook-28427.appspot.com/o/assets%2F-M_MXHpa1Cq7qojD2u_r%2F-MbI7YlHiBJg6Fg-HsOf%2F-MbIUlXZn7EYdgDNncOI%2Fanimals.zip?alt=media&token=c491c2cb-7f8b-4b23-9617-a843d38ac611\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIQf9cY6_vyn"
      },
      "source": [
        "# Unzip to './animals' folder\n",
        "!unzip -qq /content/assets%2F-M_MXHpa1Cq7qojD2u_r%2F-MbI7YlHiBJg6Fg-HsOf%2F-MbIUlXZn7EYdgDNncOI%2Fanimals.zip?alt=media"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIz-MYImAfCg"
      },
      "source": [
        "The dataset has the following folder structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuhZZqVIAqj_"
      },
      "source": [
        "animals\n",
        "- cats\n",
        "  - image_1.jpg\n",
        "  - image_2.jpg\n",
        "- dogs\n",
        "  - image_3.jpg\n",
        "  - image_4.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lez5uCJAto4"
      },
      "source": [
        "Now that you have the data, you can **create a Hub `Dataset`** and initialize its tensors. Running the following code will create a Hub dataset inside of the `./animals_hub` folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtzmT0iBNV23"
      },
      "source": [
        "import hub\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "ds = hub.empty('./animals_hub') # Creates the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ5yt0aaNeP5"
      },
      "source": [
        "Next, let's inspect the folder structure for the source dataset `'./animals'` to find the class names and the files that need to be uploaded to the Hub dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGLkgG8Njbb"
      },
      "source": [
        "# Find the class_names and list of files that need to be uploaded\n",
        "dataset_folder = './animals'\n",
        "\n",
        "class_names = os.listdir(dataset_folder)\n",
        "\n",
        "files_list = []\n",
        "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
        "    for filename in filenames:\n",
        "        files_list.append(os.path.join(dirpath, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtVSh0FnNmyI"
      },
      "source": [
        "Next, let's **create the dataset tensors and upload metadata**. Check out our page on [Storage Synchronization](https://docs.activeloop.ai/how-hub-works/storage-synchronization) for details about the `with` syntax below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6QDC6caNpiH"
      },
      "source": [
        "with ds:\n",
        "  # Create the tensors with names of your choice.\n",
        "  ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "  ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
        "\n",
        "  # Add arbitrary metadata - Optional\n",
        "  ds.info.update(description = 'My first Hub dataset')\n",
        "  ds.images.info.update(camera_type = 'SLR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD-hCSBKBA_m"
      },
      "source": [
        "**Note:** Specifying `htype` and `dtype` is not required, but it is highly recommended in order to optimize performance, especially for large datasets. Use `dtype` to specify the numeric type of tensor data, and use `htype` to specify the underlying data structure. More information on `htype` can be found [here](https://api-docs.activeloop.ai/htypes.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR4kLo6YBOhO"
      },
      "source": [
        "Finally, let's **populate the data** in the tensors.         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QRAyS-HA-Fp"
      },
      "source": [
        "with ds:\n",
        "    # Iterate through the files and append to hub dataset\n",
        "    for file in files_list:\n",
        "        label_text = os.path.basename(os.path.dirname(file))\n",
        "        label_num = class_names.index(label_text)\n",
        "        \n",
        "        ds.images.append(hub.read(file))  # Append to images tensor using hub.read\n",
        "        ds.labels.append(np.uint32(label_num)) # Append to labels tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWqYzfI1DCPG"
      },
      "source": [
        "**Note:** `ds.images.append(hub.read(path))` is functionally equivalent to `ds.image.append(PIL.Image.fromarray(path))`. However, the `hub.read()` method is significantly faster because it does not decompress and recompress the image if the compression matches the `sample_compression` for that tensor. Further details are available in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzHVb521XSud"
      },
      "source": [
        "Check out the first image from this dataset. More details about Accessing Data are available in **Step 5**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMG2oif0XSDZ"
      },
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8E_f-eXqy1c"
      },
      "source": [
        "## Automatic Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjy5dH9q3Gi"
      },
      "source": [
        "The above animals dataset can also be converted to Hub format automatically using 1 line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUtOL7F8q1xB"
      },
      "source": [
        "src = \"./animals\"\n",
        "dest = './animals_hub_auto'\n",
        "\n",
        "ds = hub.ingest(src, dest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6xboPUKrs1l"
      },
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03b3r7owq7o8"
      },
      "source": [
        "**Note**: Automatic creation currently only supports image classification datasets, though support for other dataset types is continually being added. A full list of supported datasets is available [here](https://api-docs.activeloop.ai/#hub.ingest)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK_wpkYsDdH2"
      },
      "source": [
        "## Creating Tensor Hierarchies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1btlOtBDDe4G"
      },
      "source": [
        "Often it's important to create tensors hierarchically, because information between tensors may be inherently coupledâ€”such as bounding boxes and their corresponding labels. Hierarchy can be created using tensor `groups`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICg3Z1z8CRGN"
      },
      "source": [
        "ds = hub.empty('./groups_test') # Creates the dataset\n",
        "\n",
        "# Create tensor hierarchies\n",
        "ds.create_group('my_group')\n",
        "ds.my_group.create_tensor('my_tensor')\n",
        "\n",
        "# Alternatively, a group can us created using create_tensor with '/'\n",
        "ds.create_tensor('my_group_2/my_tensor') #Automatically creates the group 'my_group_2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE-rWBCkpI9T"
      },
      "source": [
        "Tensors in groups are accessed via:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78s3Oa_jpKXV"
      },
      "source": [
        "ds.my_group.my_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fhjWZ9hDvKe"
      },
      "source": [
        "For more detailed information regarding accessing datasets and their tensors, check out the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46H4nEnZDv5m"
      },
      "source": [
        "# **Step 3**: _Understanding Compression_\n",
        "\n",
        "*Using compression to achieve optimal performance.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ajldDggEp8O"
      },
      "source": [
        "All sample data in Hub can be stored in a raw uncompressed format. However, in order to achieve optimal performance in terms of speed and memory, it is critical to specify an appropriate compression method for your data.\n",
        "\n",
        "For example, when creating a tensor for storing images, you can choose the compression technique for the image samples using the `sample_compression` input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y68OihNPPTv"
      },
      "source": [
        "import hub\n",
        "\n",
        "ds = hub.empty('./compression_test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOw9hc0jDpQY"
      },
      "source": [
        "ds.create_tensor(\"images_example\", htype = \"image\", sample_compression = \"jpeg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv4ktXoCE2K2"
      },
      "source": [
        "In this example, every image added in subsequent `.append(...)` calls is compressed using the specified `sample_compression` method. If the source data is already in the correct compression format, it is saved as-is. Otherwise, it is recompressed to the specified format, as described in detail below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WaFBxrEE9GI"
      },
      "source": [
        "#### **When choosing the optimal compression, the primary tradeoffs are lossiness, memory, and runtime:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM8VtZ98FCUu"
      },
      "source": [
        "**Lossiness** - Certain compression techniques are lossy, meaning that there is irreversible information loss when saving the data in the compressed format. \n",
        "\n",
        "**Memory** - Different compression techniques have substantially different memory footprints. For instance, `png` vs `jpeg` compression may result in a 10X difference in the size of a Hub dataset. \n",
        "\n",
        "**Runtime** - The highest uploads speeds can be achieved when the `sample_compression` value matches the compression of the source data, such as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AylKYTVCE0eI"
      },
      "source": [
        "# sample_compression and my_image are \"jpeg\"\n",
        "ds.create_tensor(\"images_jpeg\", htype = \"image\", sample_compression = \"jpeg\")\n",
        "ds.images_jpeg.append(hub.read(\"/content/animals/dogs/image_3.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hotuAwslFbAu"
      },
      "source": [
        "However, a mismatch between compression of the source data and `sample_compression` in Hub results in significantly slower upload speeds, because Hub must decompress the source data and recompress it using the specified `sample_compression` before saving:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkJKv00UFexo"
      },
      "source": [
        "# sample_compression is \"png\" and my_image is \"jpeg\"\n",
        "ds.create_tensor(\"images_png\", htype = \"image\", sample_compression = \"png\")\n",
        "ds.images_png.append(hub.read(\"/content/animals/dogs/image_3.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LMsd3K9GJJ9"
      },
      "source": [
        "**Note:** Therefore, due to the computational costs associated with decompressing and recompressing data, it is important that you consider the runtime implications of uploading source data that is compressed differently than the specified `sample_compression`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGo-E8Z8Ho6F"
      },
      "source": [
        "# **Step 4**: _Accessing Data_\n",
        "_Accessing and loading Hub Datasets._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Mye_Z5Htut"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DI_D7flHvEN"
      },
      "source": [
        "Hub Datasets can be loaded and created in a variety of storage locations with minimal configuration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9dl3mfENulO"
      },
      "source": [
        "import hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sltdan65HmRN"
      },
      "source": [
        "# Local Filepath\n",
        "ds = hub.load('./animals_hub') # Dataset created in Step 2 in this Colab Notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41FBvx25NWMN"
      },
      "source": [
        "# S3\n",
        "# ds = hub.load('s3://my_dataset_bucket', creds={...})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuacdMOgNNmT"
      },
      "source": [
        "# Public Dataset hosted by Activeloop\n",
        "ds = hub.load('hub://activeloop/k49-train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocs18sNqNQfG"
      },
      "source": [
        "# Dataset in another workspace on Activeloop Platform\n",
        "# ds = hub.load('hub://workspace_name/dataset_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD60qFaAH2qg"
      },
      "source": [
        "**Note:** Since `ds = hub.dataset(path)` can be used to both create and load datasets, you may accidentally create a new dataset if there is a typo in the path you provided while intending to load a dataset. If that occurs, simply use `ds.delete()` to remove the unintended dataset permanently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kb9q_ZqIARN"
      },
      "source": [
        "## Referencing Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq5WSI5LIClV"
      },
      "source": [
        "Hub allows you to reference specific tensors using keys or via the `.` notation outlined below. \n",
        "\n",
        "\n",
        "**Note:** data is still not loaded by these commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr_ZEtBnN1Wp"
      },
      "source": [
        "ds = hub.dataset('hub://activeloop/k49-train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24trRqlLH0Tl"
      },
      "source": [
        "### NO HIERARCHY ###\n",
        "ds.images # is equivalent to\n",
        "ds['images']\n",
        "\n",
        "ds.labels # is equivalent to\n",
        "ds['labels']\n",
        "\n",
        "### WITH HIERARCHY ###\n",
        "# ds.localization.boxes # is equivalent to\n",
        "# ds['localization/boxes']\n",
        "\n",
        "# ds.localization.labels # is equivalent to\n",
        "# ds['localization/labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjmnRLWHINXG"
      },
      "source": [
        "## Accessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3jsmBHIPqu"
      },
      "source": [
        "Data within the tensors is loaded and accessed using the `.numpy()` command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QUWjQNGILWQ"
      },
      "source": [
        "# Indexing\n",
        "ds = hub.dataset('hub://activeloop/k49-train')\n",
        "\n",
        "W = ds.images[0].numpy() # Fetch an image and return a NumPy array\n",
        "X = ds.labels[0].numpy(aslist=True) # Fetch a label and store it as a \n",
        "                                    # list of NumPy arrays\n",
        "\n",
        "# Slicing\n",
        "Y = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array\n",
        "                             # The method above produces an exception if \n",
        "                             # the images are not all the same size\n",
        "\n",
        "Z = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store \n",
        "                                        # them as a list of NumPy arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DykgrsBEIfk1"
      },
      "source": [
        "**Note:** The `.numpy()` method will produce an exception if all samples in the requested tensor do not have a uniform shape. If that's the case, running `.numpy(aslist=True)` solves the problem by returning a list of NumPy arrays, where the indices of the list correspond to different samples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQipSo2OF_lB"
      },
      "source": [
        "# **Step 5**: _Using Activeloop Storage_\n",
        "\n",
        "_Storing and loading datasets from Activeloop Platform Storage._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA39G647GHX4"
      },
      "source": [
        "You can store your Hub Datasets on Activeloop Platform by first creating an account in the CLI using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCDC-5dmGFdJ"
      },
      "source": [
        "!activeloop register"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1iZpxtOGJ0N"
      },
      "source": [
        "In order for the Python API to authenticate with the Activeloop Platform, you should log in from the CLI using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0OUCCMGGLv0"
      },
      "source": [
        "!activeloop login -u username -p password\n",
        "\n",
        "# Alternatively use \"activeloop login\" ... which is followed by prompts for username and password"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvBxhaAYGNOi"
      },
      "source": [
        "You can then access or create Hub Datasets by passing the Activeloop Platform path to `hub.dataset()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeL0a2zwGXeU"
      },
      "source": [
        "import hub\n",
        "\n",
        "# platform_path = 'hub://workspace_name/dataset_name'\n",
        "#                 'hub://jane_smith/my_awesome_dataset'\n",
        "               \n",
        "ds = hub.dataset(platform_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huQQ1M8kGcyL"
      },
      "source": [
        "**Note**: \n",
        "\n",
        "When you create an account in Activeloop Platform, a default workspace is created that has the same name as your username. You are also able to create other workspaces that represent organizations, teams, or other collections of multiple users. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUdVLQUGGnsA"
      },
      "source": [
        "Public datasets such as `'hub://activeloop/mnist-train'`  can be accessed without logging in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVma__gxGq97"
      },
      "source": [
        "# **Step 6**: _Connecting Hub Datasets to ML Frameworks_\n",
        "\n",
        "_Connecting Hub Datasets to machine learning frameworks such as PyTorch and TensorFlow._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r-AkeJMGwxB"
      },
      "source": [
        "You can connect Hub Datasets to popular ML frameworks such as PyTorch and TensorFlow using minimal boilerplate code, and Hub takes care of the parallel processing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnr9ItdkGzDk"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKkrCv2NG1GG"
      },
      "source": [
        "You can train a model by creating a PyTorch DataLoader from a Hub Dataset using `ds.pytorch()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3C2uoAGnNK"
      },
      "source": [
        "import hub\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "ds = hub.dataset('hub://activeloop/cifar100-train') # Hub Dataset\n",
        "dataloader = ds.pytorch(batch_size = 16, num_workers = 2) #PyTorch DataLoader\n",
        "\n",
        "for data in dataloader:\n",
        "    print(data)\n",
        "    break\n",
        "    # Training Loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5bX92ZUG_2F"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeRUG-arHP1F"
      },
      "source": [
        "Similarly, you can convert a Hub Dataset to a TensorFlow Dataset via the `tf.Data` API. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1bma0HSHOAO"
      },
      "source": [
        "ds # Hub Dataset object, to be used for training\n",
        "ds_tf = ds.tensorflow() # A TensorFlow Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guao84xTb4Zg"
      },
      "source": [
        "# **Step 7**: _Parallel Computing_\n",
        "\n",
        "_Running computations and processing data in parallel._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVcZ28epcKRc"
      },
      "source": [
        "Hub enables you to easily run computations in parallel and significantly accelerate your data processing workflows. This example primarily focuses on parallel dataset uploading, and other use cases such as dataset transformations can be found in [this tutorial](https://docs.activeloop.ai/tutorials/data-processing-using-parallel-computing).\n",
        "\n",
        "Parallel compute using Hub has two core elements: #1. defining a function or pipeline that will run in parallel and #2. evaluating it using the appropriate inputs and outputs. Let's start with #1 by defining a function that processes files and appends their data to the labels and images tensors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWNxzF1pcWxn"
      },
      "source": [
        "**Defining the parallel computing function**\n",
        "\n",
        "The first step for running parallel computations is to define a function that will run in parallel by decorating it using `@hub.compute`. In the example below, `file_to_hub` converts data from files into hub format, just like in **Step 2: Creating Hub Datasets Manually**. If you have not completed Step 2, please complete the section that downloads and unzips the *animals* dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjMF_-LcHtl"
      },
      "source": [
        "import hub\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "@hub.compute\n",
        "def file_to_hub(file_name, sample_out, class_names):\n",
        "    ## First two arguments are always default arguments containing:\n",
        "    #     1st argument is an element of the input iterable (list, dataset, array,...)\n",
        "    #     2nd argument is a dataset sample\n",
        "    # Other arguments are optional\n",
        "    \n",
        "    # Find the label number corresponding to the file\n",
        "    label_text = os.path.basename(os.path.dirname(file_name))\n",
        "    label_num = class_names.index(label_text)\n",
        "    \n",
        "    # Append the label and image to the output sample\n",
        "    sample_out.labels.append(np.uint32(label_num))\n",
        "    sample_out.images.append(hub.read(file_name))\n",
        "    \n",
        "    return sample_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-ZhXH-pcgT8"
      },
      "source": [
        "In all functions decorated using `@hub.compute`, the first argument must be a single element of any input iterable that is being processed in parallel. In this case, that is a filename `file_name`, becuase `file_to_hub` reads image files and populates data in the dataset's tensors. \n",
        "\n",
        "The second argument is a dataset sample `sample_out`, which can be operated on using similar syntax to dataset objects, such as `sample_out.append(...)`, `sample_out.extend(...)`, etc.\n",
        "\n",
        "The function decorated using `@hub.compute` must return `sample_out`, which represents the data that is added or modified by that function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUiNuQqchnH"
      },
      "source": [
        "**Executing the transform**\n",
        "\n",
        "To execute the transform, you must define the dataset that will be modified by the parallel computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZfEn1g_cno_"
      },
      "source": [
        "ds = hub.empty('./animals_hub_transform') # Creates the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7FIReeLcpka"
      },
      "source": [
        "Next, you define the input iterable that describes the information that will be operated on in parallel. In this case, that is a list of files `files_list` from the animals dataset in Step 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CwypbTxcrx0"
      },
      "source": [
        "# Find the class_names and list of files that need to be uploaded\n",
        "dataset_folder = './animals'\n",
        "\n",
        "class_names = os.listdir(dataset_folder)\n",
        "\n",
        "files_list = []\n",
        "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
        "    for filename in filenames:\n",
        "        files_list.append(os.path.join(dirpath, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IC-VRKVcuRI"
      },
      "source": [
        "You can now create the tensors for the dataset and **run the parallel computation** using the `.eval` syntax. Pass the optional input arguments to `file_to_hub`, and we skip the first two default arguments `file_name` and `sample_out`. \n",
        "\n",
        "The input iterable `files_list` and output dataset `ds` is passed to the `.eval` method as the first and second argument respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4H4Fug0cxJG"
      },
      "source": [
        "with ds:\n",
        "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "    ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
        "    \n",
        "    file_to_hub(class_names=class_names).eval(files_list, ds, num_workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfWc3_fkhr0W"
      },
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xTj7kt0jrd3"
      },
      "source": [
        "Congrats! You just created a dataset using parallel computing! ðŸŽˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRCphquSFs3"
      },
      "source": [
        "# **Step 8**: _Version Control_\n",
        "\n",
        "_Running computations and processing data in parallel._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_V53L8SCuB"
      },
      "source": [
        "Hub version control allows user to manage changes to datasets with commands very similar to Git. It provides critical insights into how data is evolving, and it works with datasets of any size!\n",
        "\n",
        "\n",
        "Let's create a hub dataset and check out how version control works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgEWowxySUDL"
      },
      "source": [
        "import hub\n",
        "import numpy as np\n",
        "\n",
        "# Set overwrite = True for re-runability\n",
        "ds = hub.dataset('./version_control', overwrite = True)\n",
        "\n",
        "# Create a tensor and append 200X 100x100x3 arrays\n",
        "with ds:\n",
        "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "    ds.images.extend(np.ones((200, 100, 100, 3), dtype = 'uint8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CEF-kjySdLp"
      },
      "source": [
        "##Commit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKq3VV0SdEW"
      },
      "source": [
        "To commit the data added above, simply run `ds.commit`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj9uTZeSTGwT"
      },
      "source": [
        "first_commit_id = ds.commit('Added 200X 100x100x3 arrays')\n",
        "\n",
        "print('Dataset in commit {} has {} samples'.format(first_commit_id, len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc2-MRmaSc4x"
      },
      "source": [
        "The printout shows that the first commit has 200 samples. Next, let's add 50X more samples and commit the update:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zArtG0phTZRv"
      },
      "source": [
        "with ds:\n",
        "    ds.images.extend(np.ones((50, 150, 150, 3), dtype = 'uint8'))\n",
        "    \n",
        "second_commit_id = ds.commit('Added 50X 150x150x3 arrays')\n",
        "print('Dataset in commit {} has {} samples'.format(second_commit_id, len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYjnY_1RTcjM"
      },
      "source": [
        "The printout now shows that the second commit has 250 samples. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWvgUH25Tj8V"
      },
      "source": [
        "##Log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiqOb8POTkb4"
      },
      "source": [
        "The commit history starting from the current commit can be show using `ds.log`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQSxvzIcTuU-"
      },
      "source": [
        "log = ds.log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgefyAuATwi4"
      },
      "source": [
        "This command prints the log to the console and also assigns it to the specified variable log. The author of the commit is the username of the [Activeloop account](https://docs.activeloop.ai/getting-started/using-activeloop-storage) that logged in on the machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRpqeYqV-oT"
      },
      "source": [
        "##Branch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TWcOT4RV-d4"
      },
      "source": [
        "Branching takes place by running the `ds.checkout` command with the parameter `create = True` . Let's create a new branch, add a `labels` tensor, populate it with data, create a new commit on that branch, and display the log."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY-CZmzrXr0X"
      },
      "source": [
        "ds.checkout('new_branch', create = True)\n",
        "\n",
        "with ds:\n",
        "    ds.create_tensor('labels', htype = 'class_label')\n",
        "    ds.labels.extend(np.zeros((250,1), dtype = 'uint32'))\n",
        "    \n",
        "new_branch_commit_id = ds.commit('Added labels tensor and 250X labels')\n",
        "print('Dataset in commit {} has tensors: {}'.format(new_branch_commit_id, ds.tensors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUUMXFKEXuIq"
      },
      "source": [
        "The printout shows that the dataset on the `new_branch` branch contains `images` and `labels` tensors.\n",
        "\n",
        "\n",
        "The log now shows a commit on `new_branch` as well as the previous commits on the `main`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3-UgHZPX_0u"
      },
      "source": [
        "ds.log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrKgp6FYDG9"
      },
      "source": [
        "##Checkout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07nHcIIiYFtW"
      },
      "source": [
        "A previous commit of branch can be checked out using `ds.checkout`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZe8iXjlYEdf"
      },
      "source": [
        "ds.checkout('main')\n",
        "\n",
        "print('Dataset in branch {} has tensors: {}'.format('main', ds.tensors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AZXuEVYYVHm"
      },
      "source": [
        "As expected, the printout shows that the dataset on `main` only contains the `images` tensor, since the `labels` tensor was added on `new_branch`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1GqH1JvYkNP"
      },
      "source": [
        "##HEAD Commit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbiRZ0eGiBrz"
      },
      "source": [
        "Unlike Git, Hub's version control does not have a staging area because changes to datasets are not stored locally before they are committed. All changes are automatically reflected in the dataset's permanent storage (local or cloud). **Therefore, any changes to a dataset are automatically stored in a HEAD commit on the current branch**. This means that the uncommitted changes do not appear on other branches. Let's see how this works:\n",
        "\n",
        "You should currently be on the `main` branch, which has 250 samples. Let's add 75 more samples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwuzyJUViZC6"
      },
      "source": [
        "print('Dataset on {} branch has {} samples'.format('main', len(ds)))\n",
        "\n",
        "with ds:\n",
        "    ds.images.extend(np.zeros((75, 100, 100, 3), dtype = 'uint8'))\n",
        "    \n",
        "print('After updating, the HEAD commit on {} branch has {} samples'.format('main', len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4brOnBdyiq6p"
      },
      "source": [
        "Next, if you checkout the first commit, the dataset contains 200 samples, which is sample count from when the first commit was made. Therefore, the 75 uncommitted samples that were added to the `main` branch above are not reflected when other branches or commits are checked out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvG-X9VqipM3"
      },
      "source": [
        "ds.checkout(first_commit_id)\n",
        "\n",
        "print('Dataset in commit {} has {} samples'.format(first_commit_id, len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aoAeA7vixsC"
      },
      "source": [
        "Finally, when checking our the `main` branch again, the prior uncommitted changes and visible and they are stored in the `HEAD` commit on `main`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DnXiwTmi6G9"
      },
      "source": [
        "ds.checkout('main')\n",
        "\n",
        "print('Dataset in {} branch has {} samples'.format('main', len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnTPLIS5i7yE"
      },
      "source": [
        "##Diff - Coming Soon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB79lLLJi7p4"
      },
      "source": [
        "Understanding changes between commits is critical for managing the evolution of datasets. The `diff` function will enable users to determine the number of samples that were added, removed, or updated for each tensor. Activeloop is currently working on an implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinXs4r1i7Zz"
      },
      "source": [
        "##Merge - Coming Soon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOGilvkjG2c"
      },
      "source": [
        "Merging is a critical feature for collaborating on datasets, and Activeloop is currently working on an implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz15ukH5jiIm"
      },
      "source": [
        "Congrats! You just are now an expert in dataset version control!ðŸŽ“"
      ]
    }
  ]
}